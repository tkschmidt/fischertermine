---
title: "Bavarian Fishing Exam - Free Spots Analysis"
subtitle: "Tracking exam availability across locations"
author: "Automated Analysis"
date: now
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    code-fold: true
    code-summary: "Show code"
    embed-resources: true
    page-layout: full
    fig-width: 14
    fig-height: 8
    output-file: index.html
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
#| include: false

# Load required libraries
library(data.table)
library(ggplot2)
library(lubridate)
library(jsonlite)
library(knitr)
library(DT)

# Set default theme
theme_set(theme_minimal(base_size = 12))
```

## Overview

This dashboard shows the availability of free spots for the Bavarian fishing examination across different locations over time. Data is automatically scraped every 2 hours from the [official website](https://fischerpruefung-online.bayern.de/).

```{r load-data}
# Find all JSON files in the data directory, excluding latest.json
json_files <- list.files("data", pattern = "exam-data-.*\\.json$", full.names = TRUE)
json_files <- json_files[basename(json_files) != "latest.json"]

cat("Processing", length(json_files), "JSON files...\n")

# Initialize empty list to store results
all_data <- list()

# Process each JSON file
for (json_file in json_files) {
  # Read JSON file
  data <- tryCatch({
    fromJSON(json_file)
  }, error = function(e) {
    return(NULL)
  })

  # Skip if data is NULL or missing required fields
  if (is.null(data) || is.null(data$scraped_at) ||
      is.null(data$exam_appointments) || length(data$exam_appointments) == 0) {
    next
  }

  # Extract scrape date from JSON
  scrape_date <- as.POSIXct(data$scraped_at, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")

  # Convert to data.table
  appointments <- as.data.table(data$exam_appointments)

  # Add scrape date
  appointments[, scrape_date := scrape_date]

  # Calculate free spots
  appointments[, max_participants := as.numeric(max_participants)]
  appointments[, current_participants := as.numeric(current_participants)]
  appointments[, free_spots := max_participants - current_participants]

  # Store in list
  all_data[[json_file]] <- appointments
}

# Combine all data
dt <- rbindlist(all_data, fill = TRUE)

# Create unique exam identifier (specific exam slot)
dt[, exam_id := paste(exam_date, exam_start_time, location, sep = " | ")]

# Keep only the columns we need for plotting
dt_agg <- dt[, .(exam_id, scrape_date, free_spots, max_participants, region, location, exam_date, exam_start_time)]

# Remove any potential duplicates (same exam_id at same scrape_date)
dt_agg <- unique(dt_agg, by = c("exam_id", "scrape_date"))

# Sort by exam_id and scrape_date for proper line plotting
setorder(dt_agg, exam_id, scrape_date)

# Create artificial data points for exams where first observation is below maximum
# This extends lines to show theoretical starting point when exam was released with all seats empty
first_obs <- dt_agg[, .SD[1], by = exam_id]  # Get first observation per exam (guaranteed sorted)
exams_need_artificial <- first_obs[free_spots < max_participants]

if (nrow(exams_need_artificial) > 0) {
  # Create one artificial point per exam at first scrape_date with full capacity
  artificial_points <- exams_need_artificial[, .(
    exam_id = exam_id,
    scrape_date = scrape_date,
    free_spots = max_participants,
    max_participants = max_participants,
    region = region,
    location = location,
    exam_date = exam_date,
    exam_start_time = exam_start_time
  )]

  # Combine with main dataset
  dt_agg <- rbindlist(list(dt_agg, artificial_points), use.names = TRUE)

  # Re-sort: higher free_spots first at same scrape_date to create vertical drop lines
  setorder(dt_agg, exam_id, scrape_date, -free_spots)
}

# Filter to last 3 weeks only
three_weeks_ago <- Sys.time() - weeks(3)
dt_agg <- dt_agg[scrape_date >= three_weeks_ago]
```

## Key Statistics

```{r summary-stats}
#| results: asis

total_scrapes <- uniqueN(dt_agg$scrape_date)
total_locations <- uniqueN(dt_agg$location)
date_range_start <- format(min(dt_agg$scrape_date), "%Y-%m-%d %H:%M")
date_range_end <- format(max(dt_agg$scrape_date), "%Y-%m-%d %H:%M")
total_observations <- nrow(dt_agg)

cat(sprintf("- **Total data points:** %d scrapes across %d locations\n", total_scrapes, total_locations))
cat(sprintf("- **Date range:** %s to %s (UTC)\n", date_range_start, date_range_end))
cat(sprintf("- **Total observations:** %s location-time combinations\n", format(total_observations, big.mark = ",")))
```

## Free Spots Over Time - All Locations

This chart shows the number of free exam spots across all tracked locations over time.

**Note on artificial data points:** For exams where the first observation shows fewer available seats than the exam's maximum capacity, an artificial data point is added at the same timestamp showing all seats as available. This creates a vertical drop line indicating that some seats were booked between when the exam was released and our first scrape. These artificial points help visualize the booking activity that occurred before monitoring began.

```{r plot-all-locations}
#| fig-cap: "Free spots for each exam appointment over time, split by region and location"
#| fig-height: 20

p1 <- ggplot(dt_agg, aes(x = scrape_date, y = free_spots, color = exam_id, group = exam_id)) +
  geom_line(linewidth = 0.8, alpha = 0.8) +
  geom_point(size = 1.5, alpha = 0.6) +
  facet_wrap(~ region + location, ncol = 3, scales = "free_y") +
  labs(
    title = "Free Spots Per Exam Appointment Over Time",
    subtitle = "Each line represents a specific exam (date + time), split by region and location",
    x = "Scrape Date & Time (UTC)",
    y = "Number of Free Spots"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "gray40"),
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 7),
    axis.text.y = element_text(size = 8),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    strip.text = element_text(size = 9, face = "bold")
  ) +
  scale_x_datetime(date_breaks = "12 hours",
                   date_minor_breaks = "2 hours",
                   date_labels = "%b %d\n%H:%M") +
  scale_y_continuous(limits = c(0, NA),
                     breaks = function(x) seq(0, ceiling(max(x)), by = 1))

print(p1)

# Save plot
ggsave("plots/free_spots_by_location.png", p1, width = 18, height = 24, dpi = 300, bg = "white")
```

## Exam Appointment Summary Table

Statistical summary showing minimum, average, and maximum free spots per exam appointment.

```{r summary-table}
# Create summary table for individual exams
summary_table <- dt_agg[, .(
  exam_date = first(exam_date),
  exam_time = first(exam_start_time),
  location = first(location),
  region = first(region),
  min_free_spots = min(free_spots),
  avg_free_spots = round(mean(free_spots), 1),
  max_free_spots = max(free_spots),
  observations = .N
), by = exam_id][order(-avg_free_spots)]

# Save CSV
fwrite(summary_table, "plots/location_summary.csv")

# Display interactive table
datatable(
  summary_table,
  colnames = c("Exam ID", "Exam Date", "Time", "Location", "Region",
               "Min Free Spots", "Avg Free Spots", "Max Free Spots", "Observations"),
  filter = 'top',
  options = list(
    pageLength = 25,
    autoWidth = TRUE,
    order = list(list(6, 'desc')),  # Sort by Avg Free Spots descending
    columnDefs = list(
      list(className = 'dt-center', targets = 5:8)  # Center align numeric columns
    )
  ),
  caption = "Exam appointments by average free spots (sortable and filterable)"
)
```

---

*Data source: [fischerpruefung-online.bayern.de](https://fischerpruefung-online.bayern.de/)*
*Generated: `r format(Sys.time(), '%Y-%m-%d %H:%M:%S %Z')`*
